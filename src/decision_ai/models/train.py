"""
Module `decision_ai.models.train`.

Provides functionality to optimize, train, and persist a LightGBM model
using features generated by the feature engineering pipeline.
"""
from __future__ import annotations

import joblib
from datetime import datetime
from pathlib import Path
from typing import Tuple

import optuna
import numpy as np
from lightgbm import LGBMClassifier
from sklearn.metrics import roc_auc_score
# garante que SBERTEncoder esteja disponível para deserialização do pipeline
from decision_ai.features.engineer import SBERTEncoder
from sklearn.model_selection import train_test_split

ROOT = Path(__file__).resolve().parents[2]
FEAT_DIR = ROOT / "data" / "processed" / "features"
MODEL_DIR = ROOT / "models"
MODEL_DIR.mkdir(parents=True, exist_ok=True)


def _latest(pattern: str) -> Path:
    """
    Retrieve the most recent file matching a glob pattern in the features directory.

    Parameters
    ----------
    pattern : str
        Glob pattern to match feature or pipeline artifact filenames.

    Returns
    -------
    pathlib.Path
        Path to the latest matching file.

    Raises
    ------
    FileNotFoundError
        If no files match the given pattern.
    """
    files = sorted(FEAT_DIR.glob(pattern))
    if not files:
        raise FileNotFoundError(f"No artifact matching {pattern}")
    return files[-1]


def load_features() -> Tuple[np.ndarray, np.ndarray, object]:
    """
    Load feature matrix, target labels, and preprocessing pipeline artifacts.

    Returns
    -------
    Tuple[numpy.ndarray, numpy.ndarray, object]
        Feature matrix X, label array y, and fitted preprocessing pipeline.
    """
    X = joblib.load(_latest("X_*.joblib"))
    y = joblib.load(_latest("y_*.joblib"))
    pipeline = joblib.load(_latest("pipeline_*.joblib"))
    return X, y, pipeline


def _objective(trial: optuna.Trial, X_train, X_valid, y_train, y_valid) -> float:
    """
    Objective function for Optuna hyperparameter optimization.

    Parameters
    ----------
    trial : optuna.Trial
        Optuna trial object for suggesting hyperparameters.
    X_train : numpy.ndarray
        Training feature matrix.
    X_valid : numpy.ndarray
        Validation feature matrix.
    y_train : numpy.ndarray
        Training labels.
    y_valid : numpy.ndarray
        Validation labels.

    Returns
    -------
    float
        ROC AUC score on the validation set.
    """
    # tratar desbalanço de classes
    pos = np.sum(y_train == 1)
    neg = np.sum(y_train == 0)
    scale = neg / pos if pos > 0 else 1

    params = {
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
        "num_leaves": trial.suggest_int("num_leaves", 16, 64),
        "min_child_samples": trial.suggest_int("min_child_samples", 5, 50),
        "subsample": trial.suggest_float("subsample", 0.5, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.5, 1.0),
    }
    model = LGBMClassifier(
        n_estimators=300,
        random_state=42,
        scale_pos_weight=scale,
        n_jobs=1,
        **params,
    )
    model.fit(X_train, y_train)
    preds = model.predict_proba(X_valid)[:, 1]
    return roc_auc_score(y_valid, preds)


def train(n_trials: int = 30) -> Path:
    """
    Perform hyperparameter optimization and train the final LightGBM model.

    Parameters
    ----------
    n_trials : int, optional
        Number of Optuna trials for hyperparameter search (default is 30).

    Returns
    -------
    pathlib.Path
        Path to the saved model artifact.
    """
    X, y, pipeline = load_features()
    # tratar desbalanço de classes no treino final
    pos = np.sum(y == 1)
    neg = np.sum(y == 0)
    scale = neg / pos if pos > 0 else 1

    X_tr, X_val, y_tr, y_val = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    study = optuna.create_study(direction="maximize")
    study.optimize(lambda t: _objective(t, X_tr, X_val, y_tr, y_val), n_trials=n_trials)

    best_params = study.best_params
    model = LGBMClassifier(
        n_estimators=300,
        random_state=42,
        scale_pos_weight=scale,
        n_jobs=1,
        **best_params,
    )
    model.fit(X, y)

    ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    model_path = MODEL_DIR / f"model_{ts}.joblib"
    pipe_path = MODEL_DIR / f"pipeline_{ts}.joblib"
    joblib.dump(model, model_path)
    joblib.dump(pipeline, pipe_path)
    print(f"Model saved to {model_path}")
    return model_path


# CLI entry point: parse arguments and invoke training when run as a script.
if __name__ == "__main__":  # pragma: no cover
    import argparse

    p = argparse.ArgumentParser(description="Train LGBM model")
    p.add_argument("--trials", type=int, default=30)
    args = p.parse_args()
    train(n_trials=args.trials)
